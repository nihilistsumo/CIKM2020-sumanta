\externaldocument{main}
In the early stages of the information seeking process, users are often not ready to formulate a specific search query or question; Taylor\cite{taylor2015question} refers this stage as conscious information need. In today's web search infrastructure, users with conscious needs turn towards Wikipedia, which offers articles, where multiple relevant aspects are provided in the form of sections. However many such information needs can not be satisfied with a single article and takes much effort from the user to browse through multiple articles and ranked lists of hyperlinks from search engines. While our long-term vision is to develop systems that respond to vague information needs with a Wikipedia-like article, in this work we focus on a small part of this vision: Assuming that we would be able to retrieve relevant passages for a topic, can we train an algorithm to cluster the passages into subtopics under the broad topic? \par
Researchers have explored subtopic clustering mostly in the premise of post-processing of search results in form of rankings of hyperlinks \cite{bernardini2009full, carpineto2012evaluating}. However, from their findings it is unclear how subtopic clustering can be applied to arrange passage-length texts with an ultimate goal of article construction. Also lack of suitable datasets involving passages relevant for different sub-topics makes it difficult to develop supervised models, specially neural models, for this problem. The TREC Complex Answer Retrieval (CAR) \cite{dietz2017trec} track offers a task, where for a given title and section heading, a ranking of paragraphs is to be retrieved. But in this work, we do not assume that a suitable outline is provided to us for a topic. Instead we focus on the clustering task and explore an ideal scenario where we already have the relevant passages for an Wikipedia article. Now to achieve our goal of grouping these relevant passages into subtopics, we need to develop a model that can estimate fine-grained topical differences.

This problem is particularly difficult for traditional text similarity metrics used in IR retrieval methods (BM25, tfidf, SDM) which rely on exact term matching. For example, consider the following pair of text snippets relevant for the query "Amur leopard".

\noindent\fbox{%
    \parbox{0.48\textwidth}{%
        The Amur leopard differs from other leopard subspecies by its thick fur that is pale cream-colored, particularly in winter. Rosettes on the flanks are 5 cm × 5 cm (2.0 in × 2.0 in) and widely spaced, up to 2.5 cm (0.98 in) .......
    }%
}
\noindent\fbox{%
    \parbox{0.48\textwidth}{%
        The North Chinese leopard was first described on the basis of a single tanned skin which was fulvous above, and pale beneath, with large, roundish, oblong black spots on the back and limbs, and small black spots on the head.
    }%
}

Although there is hardly any term overlap between the text snippets, it is evident that both snippets discuss the external appearance of the animal and hence should belong to the same subtopic cluster. In fact both of them are taken from the \textit{"Characteristics"} section of the original Wikipedia article titled \textit{"Amur leopard"}. Unfortunately, traditional text similarity metric such as BM25 will assign low similarity score for this pair due to lack of term matching and most likely fail to assign them into same cluster.

%Atomic unit for a typical IR task is document for which preserving sequence is not practical. Hence most of the IR methods operates in such a way that it loses the ordering information in the representation form. BOW model collapses all sequences in a document to a noisy representation which are not that sensative to fine-grained topic related tasks. Some language models like SDM does preserve small chunks of word occurences (bi-gram, tri-gram) but due to lack of long distance sequence information, it does not capture semantic or topical information very well. 

%For example: Take a document about Amur Leopard. After pre-processing and indexing, BOW representation of this document may look something like this: leopard(10), animal(9), endangered(8)....... and so on. SDM model may find ngrams such as endangered species, big cat family..... Topic model also will find similar word distributions like BOW. These information are indeed sufficient for simple IR tasks such as document retrieval with simple queries such as: leopard, endangered leopard species.... 

%Now consider a new snippet edited for this article and we have to find the suitable section where this passage can best fit into: \\

%\noindent\fbox{%
%    \parbox{0.48\textwidth}{%
%        During a study of radio-collared Amur leopards in the early 1990s, a territorial dispute between two males at a deer farm was documented, suggesting that Amur leopards favour such farms for hunting.
%    }%
%}

%This problem is particularly difficult for traditional IR retrieval methods (BM25, SDM) because it requires fine-grained topical knowledge in order to decide in which sub-topic of the article "Amur Leopard" this passage should belong to. Unsupervised similarity metrics such as TFIDF, BM25 are not sensitive enough to figure out that the sentence talks about habitat and behavior of the animal due to lack of exact term matching. 
Semantic Web-based term similarity metrics (Babelnet \cite{NavigliPonzetto:12aij}) or supervised word embeddings (GloVe \cite{pennington2014glove}) may be used to find strong relationships between the word "thick fur" in the first passage and "tanned skin" in the second and also their semantic relatedness to the "characteristics" of an animal. But meaning of a word is context dependent and if we do not take the current context into consideration, it may lead to wrong assumptions about the topic. Also access to correct contextual meaning of words does not guarantee us that we have the correct meaning of the sentence as well, because different permutation of those meanings in the sentence lead to different topical sense. Hence along with the contextual meaning of words in a sentence we also have to make sense of the particular sequence in which they are currently arranged. Moreover, a passage-sized text consists of multiple sentences, each of which may present different aspects of the same sub-topic. Together, a comprehensive meaning emerges which represents the sub-topic discussed in the passage. That is to successfully solve a difficult IR problem such as the one discussed above, a model must i) understand the semantic, contextual meaning of individual constituent units of a passage (words, sentences), ii) learn to represent the full passage in such a way that combines all semantic and sequential information provided by its constituent components.

Sequence to sequence models attempt to leverage the sequence in a natural language sentence and draws its supervision signals from that. To learn such sequence dependencies it may use different techniques such as LSTM, GRU, or Attention \cite{hochreiter1997long, cho2014learning, vaswani2017attention} but ultimately they preserve the sequence information. Transformer models which use a special type of attention mechanism called self-attention performs exceptionally well on machine translation benchmarks \cite{raganato2018analysis}. Google's BERT model which uses a bidirectional transformer, are well suited for NLP tasks \cite{devlin2018bert}. It has been applied successfully to various NLP applications such as sentence similarity, named entity recognition and question answering \cite{yang2019end, miftahutdinov2020biomedical} which used to be considered difficult for predecessors of BERT. However, there are engineering limitations for which BERT or similar models can not be used to model long sequences such as longer passages and documents. Although there are heuristics and fine-tuning modifications that allow longer inputs \cite{pappagari2019hierarchical}, it is still unclear whether these modifications are indeed necessary for passage-level IR centric tasks. In our example case, maybe using only the first few sentences as input to a BERT model is enough to model the topic of the passage. If this is the case then it will open new frontier for IR research where these state-of-the-art language understanding models find its application in solving problems that require abstract topical understanding of multi-sentence passages.

%In our example case, maybe considering only the first few sentences through BERT is enough to model the topic of the passages. But in the case of longer passages and full length documents, losing information later in the sequence will significantly hurt the performance. There are heuristics which allow longer sequences to fit into these models but they all involve complicated architectural modifications inside an already complex model. These result in an increase in training time, cost and further decrease in model explainability \ld{citation needed}. \ld{I would drop this next sentence, it is not clear what it provides and people may feel offended:} Also engineers will be reluctant to welcome any major architectural changes to the core components of a downstream task pipeline that has incorporated BERT based model and had already fine-tuned its hyper-parameters. All these reasons severely limit the applicability of transformer-based models into difficult IR tasks.\ld{This is all nice text, but I don't really undertand how your argument follows that this will not be good for IR -- after all, it is good for NLP! Also keep in mind that whatever you say here will be used to measure your own work. So the strategy is to be humble and factual.}

So a clear gap of research has emerged from the previous discussion: Transformer models which have been successful in solving numerous NLP problems can not be applied to diffiuclt IR problems such as subtopic clustering which can surely benefit from a well-suited language understanding model. We bridge this gap by utilizing Transformer-based embedding models with novel modifications that allow us to model topical similarity between passage-sized texts. A thorough evaluation of our passage embedding methods show that BERT-like complex models can be adapted to subtopic clustering without complex modifications to fine-tuning step or any architectural changes.

%So we identify two major complementary avenues of research: \ld{it was unclear what option 1 and 2 are, I placed numbers, hoping it would help, but I am not sure I placed them right} (1) Transformer models which have been successful in solving numerous NLP problems and (2) difficult IR problems such as subtopic clustering which can surely benefit from a well-suited language understanding model such as BERT \ld{I don't get how "difficult IR problems" is an avenue for research}. We bridge this gap \ld{which gap is that?} by utilizing Transformer-based embedding models with novel \ld{I dropped "simple yet powerful"} modifications that allow us to embed passage-sized texts. A thorough evaluation of our passage embedding methods show that BERT-like complex models can be adapted to \ld{Drop: "difficult IR tasks such as"} subtopic clustering without major architectural changes. 
The contributions of this work are as follows:

\begin{itemize}[leftmargin=.15in]
    \item Explore simple modifications to BERT-based sentence embedding methods that can be trained and used to embed passage-sized texts.
    \item We propose CATS: Context Aware Triamese Similarity network, a supervised similarity metric which calculates the similarity score between a pair of passage embeddings while taking the context into consideration.
    \item Using the proposed modifications, we present a solution to the subtopic clustering problem.
\end{itemize}

This paper is organized in following sections: Section \ref{sec:rel} describes related work in this area along with some background on sentence BERT embeddings. Section \ref{sec:app} describes our approach in detail. Section \ref{sec:eval} describes our experimental settings and discuss the findings from our empirical study. Finally, in Section \ref{sec:con} we conclude our work.

%(Word is atomic unit for sentence but when I'm trying to work with passages, sentence should be atomic unit. <- this can be justification for MaLSTM sentence embedding
%Also the whole passage text can be treated as a long sequence and initial 512 tokens may be enough to decide the sub-topic of the passage. Hence if the whole passage is not very much beyond the max sequence then we should choose this approach. However, if it is not the case then the previous approach should be used. We can use the two experiments with by1test and 4sub to show that sentwise embeddings are more effective than whole passage embedding for longer passage sequence.)
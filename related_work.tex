\subsection{Background on sentence BERT embeddings}
It is observed in deep learning and transfer learning research that layers at different levels of a deep network capture specific information about the data \cite{peters2018dissecting}. In their work, Peters et al. \cite{peters2018deep} learned a function that projects internal state of a deep Bidirectional Language Model which was trained on large dataset to a contextual embedding space. They released this embedding vectors called ELMo which proved to be useful across many NLP problems. Being a deep network, BERT \cite{devlin2018bert} also has several layers of attention heads and feed-forward neural networks stacked on top of each other. Researchers have tried to utilize information captured at these layers by averaging all BERT layers \cite{zhang2019bertscore} or extracting the output of a special token (CLS) in the input \cite{may2019measuring}\cite{qiao2019understanding} to obtain a fixed size embedding representing the input sequence. Unfortunately empirical studies prove that these methods perform poorly in semantic matching tasks. This motivated Reimers et al. \cite{reimers2019sentence} to come up with some modifications in retrieving strategies of these embeddings as well as specific fine-tuning techniques that provide better sentence embeddings in terms of its performance in numerous sentence similarity tasks. The key differences between their approach and simpler approaches attempted before them are the following:

1. Averaging hidden layers tend to lose vital semantic information captured in separate layers of BERT. Instead Reimers et al. experiment with three pooling strtegies: CLS, max pooling, and mean pooling. 

2. Also they use two different network structures to fine-tune these embeddings: Siamese networks for pairwise training data (binary similarity regression and multiclass classification) and triplet network for sentence triples (one anchor, similar, and not similar).

3. They evaluate three different loss functions which will tune the embeddings for semantic similarity tasks. a) regression loss minimizes mean-squared error (MSE) between the similarity labels and cosine similarity between sentence embeddings, b) classification loss optimizes cross-entropy loss between weighted class probabilities from a concatenation of embeddings of sentence pair, c) triplet loss trains the network such that the distance between anchor sentence and positive sentence is higher than that of negative sentence.

Although the authors claim that they have devised these methods to help clustering and semantic search tasks, they only apply these methods to embed sentences. As a consequence they  only evaluated their methods on sentence-level tasks. As described before, IR tasks require a representation of the larger context beyond a single sentence. It is hence unclear whether these embedding strategies are applicable to the \ld{paragraph instead of passage? Some people define passage as sentence} passage- or document-level;  and what necessary modifications are necessary.

\subsection{Other related work} Previous research work in text clustering\cite{kulis2009semi,bilenko2004integrating,davidson2008finding,basu2004probabilistic,basu2002semi,gomaa2013survey} focused on unsupervised lexical similarity metrics as the distance function used by hierarchical agglomerative clustering algorithms. Metzler et al.\cite{metzler2007similarity} explore some hybrid similarity measures which combine lexical and probabilistic measures with application to query similarity detection. Banea et al.\cite{banea2012unt} develop a synergistic combination of text similarity measures. For semi-supervised clustering, researchers have found pairwise binary constraints also known as \textit{``must link"} and \textit{``can not link"} to be particularly effective. Most lexical similarity metrics work with a term-based vector representation of text such as TFIDF. However, Steyvers et al.\cite{steyvers2007probabilistic} propose to measure document similarity by first inferring the topic distributions of documents by LDA\cite{blei2003latent} topic modeling and then calculating the Kullback-Leibler (KL) divergence between them. Arnold et al. propose SECTOR \cite{arnold2019sector} which segments a long-piece of text into coherent topical segments using deviation of topic embeddings.

Raiber et al. \cite{raiber2013ranking} has showed how to employ query-specific clustering methods in document ranking tasks using Markov Random Fields to improve diversity in search results. Bernardini et al. \cite{bernardini2009full} cluster search results for a broad topic into subtopics using keyphrases. Carpineto et al. \cite{carpineto2012evaluating} evaluate the effectiveness of sub-topic clustering and diversification in post-processing search results.

Representing text plays an important role in effective clustering. Li et al. \cite{li2016generative} combine topic modeling and word embedding techniques to represent documents that capture topical information. They claim that their model can learn topical representation vectors using significantly less data than traditional topic models. As they evaluate their model on retrieval tasks with highly distinct topical centroids (on 20 Newsgroup and Reuters benchmarks), it is unclear how accurate their model will be in capturing fine-grained topics necessary for IR tasks. When Mikolov et al. \cite{mikolov2013distributed} introduced their word2vec model and Pennington et al. \cite{pennington2014glove} released their implementation of global word embeddings (GloVe), researchers have tried to use average word vectors to represent longer sequences of texts such as a sentence. However, the results tend to decrease drastically for passages and even short documents due to increased noise while averaging. There are works which try to embed longer text units than words in semantic-rich fashion. Most notable work on this direction must be the paragraph embeddings by Le et al. \cite{le2014distributed}. They propose the idea of representing variable length texts with a fixed size vector which can capture semantic distance between texts much like word vectors. Dai et al. \cite{dai2015document} not only broaden the scope of paragraph vectors but also propose modifications that improve embedding quality. Skip-thought algorithm proposed by Kiros et al. \cite{kiros2015skip} extended the skip-gram model described in the word2vec paper in sentence-level to obtain sentence vectors. Hill et al. \cite{hill2016learning} propose fastSent and Sequential Denoising Autoencoder which are similar to skip-thought but much more computationally efficient. They conclude that optimal sentence representation model depends largely on the nature of the downstream tasks: supervised, unsupervised or semi-supervised application domain. However they focus only on sentence-level tasks and it is unclear how these models will perform when applied to paragraphs.
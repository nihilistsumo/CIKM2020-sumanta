Transformer based embedding models have been applied successfully solve sentence or phrase similarity tasks. On the other hand, to solve difficult IR-centric problems such as subtopic clustering, we require contextual understanding of long-form texts that Transformer models such as BERT can provide. But several engineering limitations prevented usage of these models for longer sequence of texts such as passage or documents. We develop simple yet powerful techniques to adapt Transformer based embedding models to generate passage embeddings, suitable for capturing subtopic similarity. We also propose CATS, a Triamese neural architecture that models contextual similarity function between passage embedding vectors. Our exhaustive empirical findings show that our Transformer based passage embeddings in tandem with CATS, outperform strong BERT baseline models and therefore can be used as a simple and better solution to the subtopic clustering problems.
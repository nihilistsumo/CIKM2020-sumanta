We require machine understanding of long-range contexts to solve difficult IR-centric problems such as subtopic clustering. Empirical results have shown that Transformer-based embedding models capture rich context information in short text sequences such as sentences and have been applied successfully for sentence or phrase similarity tasks. However, several engineering challenges limit the usefulness of these models for longer sequence of texts such as passage or documents. We develop simple yet effective techniques to adapt Transformer-based embedding models to generate passage embeddings, suitable for capturing subtopic similarity. We also propose CATS, a Triamese neural architecture that models contextual similarity function between passage embedding vectors, with a dependence on the user query. Our exhaustive empirical findings show that our Transformer-based passage embeddings in combination with CATS, outperform pure BERT baseline models and therefore provides better solution to the subtopic clustering problems.
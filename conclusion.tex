Transformer-based embedding models have been applied successfully for sentence or phrase similarity tasks. On the other hand, to solve difficult IR-centric problems such as subtopic clustering, we require machine understanding of long-range contexts \ld{what is the different between the rest of this sentence and the beginning - are you missing a 'not'? or maybe I am not understanding...} that Transformer models such as BERT can provide. However, several engineering challenges limit the usefulness of these models for longer sequence of texts such as passage or documents. We develop simple yet powerful techniques to adapt Transformer-based embedding models to generate passage embeddings, suitable for capturing subtopic similarity. We also propose CATS, a Triamese neural architecture that models contextual similarity function between passage embedding vectors, with a dependence on the user query. Our exhaustive empirical findings show that our Transformer-based passage embeddings in combination with CATS, outperform strong BERT baseline models and therefore can be used as a simple and better solution to the subtopic clustering problems.
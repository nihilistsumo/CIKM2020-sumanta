\subsection{Experimental settings} The central theme of our work is to explore an efficient way to make use of embeddings and transformer models to represent passage-length texts. Our experimental setting evaluates to which extent such a passage representations is useful for solving passages clustering in an open-domain IR context. \ld{I would reword this: "We chose sub-topic clustering of passages as our evaluation problem because traditional IR similarity metrics have been shown to be ineffective in modeling the fine-grained similarity between passages from the same broad topic and hence is considered to be a difficult IR problem." - see ticket \url{https://gitlab.cs.unh.edu/trema/papers/cikm2020-subtopic-clustering/issues/4}} However as mentioned before, we train our models to learn query-dependent pairwise distances between passages first, and then use the pairwise distances for clustering. We describe our clustering pipeline in the following:
\begin{itemize}
    \item \textbf{Step 1: \ld{I added a few words, and maybe made the name too long, please fix if necessary} Query-dependent binary classification  into same/different} We train our model phrasing the model of query-dependent decision into same/different sections as a binary classification problem. Given a pair of passages $p_1, p_2$  \ld{... and a query...} from an Wikipedia article, we have to decide whether they should share the same section in that article or not. We use the TRECCAR-Train as our training data.
    \item \textbf{Step 2: Pairwise distances \ld{1 is also pairwise distances, maybe you want to call this ``Supervised Distance Combination Learning'' ?}} 
    \ld{I don't know what you are trying to say, I suspect you are training a combination of different similarity metrics (a la Step 1). I would phrase this as follows: ``Since we have different methods for estimating a query-independent as well as  query-specific pair-wise similarity metrics (some supervised, some unsupervised like TF-IDF), we learn a model combination.''}
    Using the trained model from previous step or other unsupervised models, we calculate pairwise distances between all passage pairs which we want to cluster. If a passage embedding model is being trained (e.g., ews or ewp), then a distance function (cosine distance or CATS) is used to generate passage similarity scores between pairs of passage embedding vectors. For CATS, it is trained again for binary classification task using TRECCAR-by1train dataset.
    \item \textbf{Step 3: Subtopic clustering} Given a set of passages $P$ about a topic $T$, we have to generate $k$ clusters $c_1 .. c_k$ of passages, in such a way that each cluster $c_i$ coincides with a specific sub-topic of $T$. Using the pre-computed pairwise distances as input, we use Hierarchical Agglomerative clustering with average linkage to obtain clusters of passages from same article.
\end{itemize}

The training and evaluation phase of the clustering pipeline is depicted in Figure \ref{fig:pipe}.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{graphics/pipeline2.png}
  \caption{Training and evaluation phase of clustering pipeline}
  \label{fig:pipe}
\end{figure}

\subsection{Training and evaluation data} 
We use several parts of a publicly available TREC Complex Answer Retrieval dataset for training and evaluating our models: benchmarkY1-train.v2.0 (trec y1 train), benchmarkY1-test (trec y1 test), benchmarkY2test-auto-manual-qrels (trec y2 test), and train.v2.0 (trec train) \ld{this enumeration needs a bit more explanation}. The dataset consists of a large corpus of text passages extracted from Wikipedia and text book articles \ld{I deleted TQA, not sure if you use the term later}. One of the target task described by the organizers of the CAR challenge  is a passage retrieval task where we have to retrieve relevant passages from the corpus for complex queries. The query set is derived from concatenating section headings of an article with its article title. The automatic benchmark will, for such queries, only consider passages as relevant if they originate from the corresponding section of the article. This relevance information is used to obtain the hierarchical structure of each article in the dataset; these are used as input to our training data generation method depicted in Figure \ref{fig:conv}. \ld{this needs to also be explained in prose} The process produces passage similarity data for training the query-dependent binary classification task and is used in step 1 (and step 2 for CATS) of our subtopic clustering pipeline.

%Each part has ground truth information contained in qrels files which inform about relevance of passages to different hierarchical sections of an Wikipedia article. For trec y1 parts, it has two different levels of hierarchical relevance: hierarchical qrels considers each section in Wikipedia articles as different topics and maps relevant passages accordingly. Toplevel qrels does not consider deeper level sections and maps all passages from deeper level sections to the section on the topmost level of the section hierarchy.

\subsubsection{Training data} Using the technique depicted in Figure \ref{fig:conv}, we convert subset of trec train and trec y1 train dataset from TRECCAR to our training sets \textbf{Train-TRECCAR} and \textbf{by1train-TRECCAR} respectively. We also generate two \ld{make clear that the query is separate from the triplet, because earlier you talked about tuples (u,v,w), where one was the query} triplet dataset to fine-tune \ld{I would say a model similar to the one from} Dor et al. version of text embedding model. The dataset \textbf{Train-triplet-psg} contains samples of passage triplets as $(p_a, p_s, p_d)$ where $p_a$ and $p_s$ are similar and $p_d$ is dissimilar according to Train-TRECCAR. \textbf{Train-triplet-sent} is constructed similar to Train-triplet-psg except we take a random sentence from each passage in a triplet instead of the whole passage text. \ld{make sure the math notation is in line with what was used earlier.}

\subsubsection{Evaluation data} We evaluate our models on both the query-specific binary classification task and the clustering task. The former task is evaluated on \textbf{Y1 test} and \textbf{Y2 test}, converted from trec y1 test and trec y2 test, similar to \textbf{Train-TRECCAR} \ld{For an outsider it is difficult to follow what the difference is between "Y1 test" and "trec y1 test". Maybe you can simplify the terminology?}. For the clustering task, we use the section-passage relevance information provided in the automatic ground truth of TREC CAR dataset to construct the clustering ground truth of true passage clusters in articles. However, based on topical diversity inside a cluster, we construct two different clustering ground truths:

\begin{itemize}
    \item \textit{Lenient} We consider all passages under the same top-level section to be a single cluster. For example: passages relevant for \textit{Amur Leopard/Threats}, \textit{Amur Leopard/Threats/Forest degradation}, and \textit{Amur Leopard/Threats/Poaching} are considered part of same \textit{"Threat"} cluster.
    \item \textit{Hierarchical} We consider passages under different hierarchical sections to be separate clusters. In previous example, we would form three distinct clusters (\textit{"Threats", "Forest degradation",} and \textit{"Poaching"}) for the \textit{"Amur Leopard"} article.
\end{itemize}

\subsection{Experimental models} We experiment with the following models:
\begin{itemize}
    \item \textbf{BERT models} BERT base cased (bert-c) and BERT base uncased (bert-u) pretrained models finetuned using Train-TRECCAR.
    \item \textbf{Passage embedding models} Four variations of Sentence-BERT models are used to generate passage embeddings: 
    \begin{itemize}
        \item \textit{en:} BERT-base model with mean-tokens pooling trained on SNLI and MultiNLI dataset.
        \item \textit{ens:} Same as en but first fine-tuned on the AllNLI datasent, then on train set of STS benchmark.
        \item \textit{ews:} Version of Dor et al. text embeddings which are fine-tuned on triplets generated from Wikipedia sections. However to prevent training data leakage, we fine-tuned the model using sentence triplets generated from Train-TRECCAR (called Train-triplet-sent), instead of the training dataset provided by the authors \ld{if you mean me, than call me "track organizer" of you mean Dor, then explain why you are not using their models (e.g., because their application differs)}.
        \item \textit{ewp:} Same as ews but the model is trained on triples of passages instead of sentences (called Train-triplet-psg) from Train-TRECCAR.
    \end{itemize}
    Each of the models above are used to generate passage embeddings in two ways: \textit{I. } By using the whole passage text as a single long input sequence and \textit{II. } By averaging sentence embeddings generated for each sentence of a passage. 
    \item \textbf{CATS} We train our CATS model discussed in Section \ref{sec:cats}, for each of the passage embedding methods using by1train-TRECCAR dataset. We use the embedding vector of the article title as the as the query-specific context information for CATS, as intended by the CAR track organizers.
    \item \ld{add LDA, and add tf-idf cosine similarity, and BM25}
\end{itemize}

We refer to all passage embedding models by the following simple nomenclature: \texttt{EMB-GEN-DIST} where $EMB$ denotes the embedding model used ($en/ens/ews/ewp$), $GEN$ denotes whether we use the whole passage text or individual sentences as input while generating the embeddings ($psg/sent$) and finally $DIST$ denotes which type of distance function we use to obtain the similarity scores between two passage embedding vectors ($cos/CATS$).

\subsection{Subtopic clustering} For each article in \textbf{Y1 test} and \textbf{Y2 test}, we cluster all passages from the article using our clustering pipeline discussed before. The clustering results are evaluated using mean Adjusted RAND index \ld{I think we should give the equation of ARI, or at least say that it is precision/F1 on whether two passages were assigned correctly to the same/different subtopics} across all the articles in each dataset. For Y1 test, we evaluate the clusters against both the \textit{lenient} and \textit{hierarchical} clustering ground truth and for Y2 test, we only evaluate on \textit{lenient} ground truth. For each case, we also evaluate two scenarios: the true number of clusters for each article is i) provided ii) not provided, so a fixed cluster number is predetermined. The supervised methods were actually trained on binary classification task, so we also report the binary classification evaluation scores in terms of AUC score (Area under the receiver operating characteristics curve). Please refer to Table \ref{tab:by1} for the evaluation results.

\begin{table*}[t]
\centering
%\ra{1.3}
\caption{\ld{I love the table. Can you mark which method is "ours"?}Task performance measured in terms of macro averaged evaluation score and paired ttest with respect to $bertu$ with $\alpha = 0.05$}
\label{tab:by1}
\begin{tabular}{@{}lrrrrrrrrr@{}}\toprule
& \multicolumn{5}{c}{Y1 test} && \multicolumn{3}{c}{Y2 test}  \\
\cmidrule(r){2-6}
\cmidrule(r){8-10}
%\cmidrule{2-6} 
Methods & AUC & hq ARI (k=6) & hq ARI (k=true) & ln ARI (k=6) & ln ARI (k=true) && AUC & ARI (k=8) & ARI (k=true) \\\midrule
bm25 & 0.7109 & 0.0920 & 0.1771 & 0.1227 & 0.1451 && 0.6301 & 0.1571 & 0.1318 \\
LDA & & & & & && & & \\
bertc & 0.7771 & 0.1373 & 0.2300 & 0.1797 & 0.2078 && 0.6920 & 0.2275 & 0.2368 \\ 
bertu & 0.7778 & 0.1303 & 0.2140 & 0.1789 & 0.1912 && 0.6876 & 0.1824 & 0.1821 \\\midrule
ews-psg-cos & 0.7910 & 0.1451 & 0.2085 & $\blacktriangle$ 0.2302 & $\blacktriangle$ 0.2397 && $\blacktriangle$ \textbf{0.7194} & $\blacktriangle$ 0.2261 & $\blacktriangle$ 0.2422 \\ 
ewp-psg-cos & $\blacktriangle$ 0.7995 & 0.1502 & 0.2074 & $\blacktriangle$ 0.2245 & $\blacktriangle$ 0.2503 && $\blacktriangle$ 0.7089 & $\blacktriangle$ 0.2574 & $\blacktriangle$ 0.2261 \\ 
en-psg-cos & $\blacktriangledown$ 0.6686 & $\blacktriangledown$ 0.0559 & $\blacktriangledown$ 0.1018 & $\blacktriangledown$ 0.0771 & $\blacktriangledown$ 0.0910 && $\blacktriangledown$ 0.6302 & $\blacktriangledown$ 0.1041 & $\blacktriangledown$ 0.0990 \\ 
ens-psg-cos & $\blacktriangledown$ 0.6727 & $\blacktriangledown$ 0.0575 & $\blacktriangledown$ 0.1073 & $\blacktriangledown$ 0.0701 & $\blacktriangledown$ 0.0883 && $\blacktriangledown$ 0.6261 & $\blacktriangledown$ 0.0899 & $\blacktriangledown$ 0.0963 \\\midrule
ews-sent-cos & $\blacktriangledown$ 0.7379 & $\blacktriangledown$ 0.0882 & $\blacktriangledown$ 0.1564 & 0.1487 & 0.1708 && 0.6815 & 0.1515 & $\blacktriangledown$ 0.1326 \\ 
ewp-sent-cos & $\blacktriangledown$ 0.7350 & $\blacktriangledown$ 0.0951 & $\blacktriangledown$ 0.1589 & 0.1530 & 0.1663 && 0.6713 & $\blacktriangledown$ 0.1164 & $\blacktriangledown$ 0.1172 \\ 
en-sent-cos & $\blacktriangledown$ 0.6629 & $\blacktriangledown$ 0.0644 & $\blacktriangledown$ 0.1118 & $\blacktriangledown$ 0.0918 & $\blacktriangledown$ 0.0971 && $\blacktriangledown$ 0.6313 & $\blacktriangledown$ 0.1119 & $\blacktriangledown$ 0.1075 \\ 
ens-sent-cos & $\blacktriangledown$ 0.6728 & $\blacktriangledown$ 0.0598 & $\blacktriangledown$ 0.1037 & $\blacktriangledown$ 0.0856 & $\blacktriangledown$ 0.0929 && $\blacktriangledown$ 0.6280 & $\blacktriangledown$ 0.0801 & $\blacktriangledown$ 0.0797 \\\midrule
ews-psg-CATS & $\blacktriangle$ 0.7953 & $\blacktriangle$ 0.1623 & 0.2289 & $\blacktriangle$ 0.2445 & $\blacktriangle$ 0.2698 && $\blacktriangle$ 0.7173 & $\blacktriangle$ 0.2401 & $\blacktriangle$ \textbf{0.2517} \\ 
ewp-psg-CATS & $\blacktriangle$ \textbf{0.8045} & $\blacktriangle$\textbf{0.1742} & \textbf{0.2380} & $\blacktriangle$ \textbf{0.2605} & $\blacktriangle$ \textbf{0.2772} && $\blacktriangle$ 0.7097 & $\blacktriangle$ \textbf{0.2640} & $\blacktriangle$ 0.2319 \\ 
en-psg-CATS & $\blacktriangledown$ 0.7089 & 0.1106 & $\blacktriangledown$ 0.1382 & 0.1657 & 0.1608 && $\blacktriangledown$ 0.6520 & 0.1688 & 0.1623 \\ 
ens-psg-CATS & $\blacktriangledown$ 0.7094 & 0.1098 & $\blacktriangledown$ 0.1522 & 0.1561 & 0.1639 && $\blacktriangledown$ 0.6441 & 0.1533 & 0.1573 \\\midrule
ews-sent-CATS & 0.7673 & 0.1451 & 0.2041 & $\blacktriangle$ 0.2331 & $\blacktriangle$ 0.2481 && $\blacktriangle$ 0.7145 & 0.2140 & $\blacktriangle$ 0.2242 \\ 
ewp-sent-CATS & 0.7754 & 0.1539 & 0.2136 & $\blacktriangle$ 0.2314 & $\blacktriangle$ 0.2354 && 0.6995 & 0.2008 & 0.2024 \\ 
en-sent-CATS & $\blacktriangledown$ 0.7032 & $\blacktriangledown$ 0.0931 & $\blacktriangledown$ 0.1317 & $\blacktriangledown$ 0.1461 & 0.1673 && $\blacktriangledown$ 0.6572 & 0.1597 & 0.1563 \\ 
ens-sent-CATS & $\blacktriangledown$ 0.7082 & 0.1041 & $\blacktriangledown$ 0.1484 & 0.1512 & 0.1663 && $\blacktriangledown$ 0.6416 & 0.1624 & 0.1449 \\ 
\bottomrule
\end{tabular}
\caption*{Significantly higher $\blacktriangle$ or lower $\blacktriangledown$ than $bertu$ according to paired t-test \\
hq: \ld{why is it hq and not hc? ist is because of hierarchical queries how about hierarchical secions (hs)? }  evaluated against hierarchical clustering ground truth, ln: lenient clustering ground truth\\
k: number of clusters, k=true: original number of clusters in each article}
%\vspace{-3ex}
\end{table*}

\subsubsection{Observation} From Table \ref{tab:by1}, we make the following interesting observations:
\begin{itemize}
    \item We can see that in six out of eight cases, ewp-psg-CATS is the best performing method. All of these cases except one are statistically significant when compared to the uncased BERT method (bertu), a strong baseline, according to paired ttest.
    \item All four passage embedding methods benefit from using CATS as the distance function over cosine similarity. For example: ewp-psg-CATS improves about $15$\% over ewp-psg-CATS in terms of mean ARI score when evaluated against hierarchical ground truth of Y1 test with true number of clusters provided (column h ARI(k=true)). This improvement is not only observed for all embedding methods but across all evaluation settings as well.
    \item The CATS variant of $en, ens$ improves remarkably over their cosine similarity versions. In some cases the improvement is over $100$\%. For example: en-psg-CATS improves about $115$\% over en-psg-cos in terms of mean ARI when evaluated against lenient clustering ground truth of Y1 test with fixed number of clusters (column ln ARI(k=6)).
    \item Using the average sentence vector (sent) to represent passages is observed to produce worse outcome when compared to whole passage embedding (psg) for $ewp, ews$. For example: ews-sent-cos performs about $7$\% worse than ews-psg-cos in terms of AUC score for Y1 test. However, in most cases we observe slight performance increase for $en, ens$ when average sentence vector is used.
\end{itemize}

\ld{awesome findings! I actually like that you formatted it with bullet points.}

\subsubsection{Significance test} We have seen from Table \ref{tab:by1} that our proposed CATS method improves all four passage embedding methods explored here. Now to investigate whether the improvement is statistically significant, we conduct paired ttest between each CATS version of passage embedding method and its cosine version. We choose $\alpha = 0.05$ for this study. For brevity, we consider three different cases: Y1 test with hierarchical and lenient ground truth, and Y2 test, both with true number of clusters provided. We report the obtained p-value from the experiments in Table \ref{tab:sigt}. The results confirm that all improvements due to CATS are statistically significant for Y1 test dataset. Whereas for Y2 test, improvements for $en, ens$ methods are statistically significant.

\ld{I like this discussion on significance, but it really will raise the question on whether you need a "correction for multiple hypothesis testing" also called Bonferroni correction -- its just some more math on top of the p-value, we just need to look up how it works exactly.}


\begin{table}[t]
\centering
%\ra{1.3}
\caption{Significance test of passage embedding methods with respect to its corresponding CATS version with $\alpha = 0.05$}
\label{tab:sigt}
\begin{tabular}{@{}lrrrr@{}}\toprule
&& \multicolumn{2}{c}{Y1 test} & Y2 test  \\
\cmidrule(r){3-4}
\cmidrule(r){5-5}
Method 1 & Method 2 & hq k=t & ln k=t & k=t \\\midrule
ews-psg-cos & ews-psg-CATS & 0.0386 & 0.0138 & 0.5887 \\ 
ewp-psg-cos & ewp-psg-CATS & 0.0007 & 0.023 & 0.7462 \\ 
en-psg-cos & en-psg-CATS & 0.0002 & <0.0001 & 0.0174 \\ 
ens-psg-cos & ens-psg-CATS & <0.0001 & <0.0001 & 0.0217 \\
ews-sent-cos & ews-sent-CATS & 0.0001 & <0.0001 & 0.0004 \\
ewp-sent-cos & ewp-sent-CATS & <0.0001 & <0.0001 & 0.0016 \\
en-sent-cos & en-sent-CATS & 0.0364 & <0.0001 & 0.0684 \\
ens-sent-cos & ens-sent-CATS & 0.0001 & <0.0001 & 0.0016 \\
\bottomrule
\end{tabular}
\caption*{hq: evaluated against hierarchical clustering ground truth, ln: lenient clustering ground truth\\
k: number of clusters, k=true: original number of clusters in each article}
%\vspace{-3ex}
\end{table}

One important aspect to note here is that the adjusted RAND index score has non-linear relationship with correct cluster assignment of data points. To be more precise, moving a single data point from an incorrect cluster to a correct cluster adds to the true positive and true negative components by the cardinality of the two clusters involved.  Also it increases slowly when most of the data points are initially in the wrong clusters. From the clustering point of view, when the initial cluster is having a very low Adjusted RAND index, significant improvement on the quality of clustering may not yield similar improvement in ARI score when compared to an already near-perfect initial cluster. As our best baseline clustering result yields an ARI score of only around 0.2, any minor improvement in ARI score by our proposed method may suggest significant improvement on the quality of resulting clusters whether or not it is shown in the significance tests. Also due to the same reason, when there are very few true number of clusters or very few data points in each cluster in a clustering problem, misplacing a single data point into a wrong cluster will significantly degrade the clustering performance. Hence the margin of error in this scenario is too low to achieve a high ARI score even for a decent clustering method. The datasets used here has only 7 sections on average for each article and each section has around 3 to 7 passages on average. This explains the low ARI scores achieved across all the methods for this dataset.

\subsubsection{Ablation study} As discussed in Section \ref{sec:cats} and Figure \ref{fig:triam}, final similarity score is learnt from the resulting vector after concatenating transformed query vector, both passage vectors and their difference vectors. To investigate the relative importance of each of these six components, we carry out an ablation study. In these study we take $ewp-psg-CATS$ and measure its performance after removing different combinations of vectors from the concatenation step. Please refer to Table \ref{tab:abl} for the results. For brevity, we report only the three cases used for the significance test discussed previously. 

\begin{table}[t]
\centering
%\ra{1.3}
\caption{Ablation study of different components for concatenation}
\label{tab:abl}
\begin{tabular}{@{}lrrr@{}}\toprule
& \multicolumn{2}{c}{Y1 test} & Y2 test  \\
\cmidrule(r){2-3}
\cmidrule(r){4-4}
Combination & hq k=t & ln k=t & k=t \\\midrule
1. v',w' & 0.0207 & 0.0382 &  \\ 
2. v',w',(v'-w') & 0.2198 & 0.2523 & \\
3. u',v',w' & 0.0328 & 0.0415 & \\
4. u',v',w',(v'-w') & 0.2198 & 0.2690 & \\
5. u',v',w',(v'-u'),(w'-u') & 0.0359 & 0.0447 & \\
6. v',w',(v'-u'),(w'-u') & 0.0290 & 0.0434 & \\
7. v',w',(v'-w'),(v'-u'),(w'-u') & 0.2429 & 0.2795 & \\
8. (v'-w'),(v'-u'),(w'-u') & 0.2193 & 0.2639 & \\
9. (v'-u'),(w'-u') & 0.0281 & 0.0469 & \\
\textbf{10. u',v',w',(v'-w'),(v'-u'),(w'-u')} & 0.2380 & 0.2772 & 0.2319 \\
\bottomrule
\end{tabular}
\caption*{Bold text refers to the combination used for our experiments \\
u',v',w': Refer to Figure \ref{fig:triam}}
%\vspace{-3ex}
\end{table}

From the result we can make the following observations:
\begin{itemize}
    \item Combination 1 and 2 do not involve context vector (u') and both of them performs worse than many other combinations involving context vector. Hence we can conclude that including context vector positively impacts model's ability to estimate topical similarity between passage embedding vectors.
    \item Another interesting observation we make from Table \ref{tab:abl} is the fact that difference vectors are more helpful than original embedding vector. For example, inclusion of the difference vector between two passage embedding vectors (v'-w') in Combination 4 along with all three embedding vectors (u',v',w') resulted in large performance gain over Combination 3. In fact Combination 8 which uses only the difference vectors, yields comparable results with best performing methods.
    \item We observe that Combination 7 obtains slightly better results than Combination 10 (all six components) which is used for our clustering study. This suggests that the success of CATS comes from the difference vectors between passage and context, and not from the context vector itself. 
\end{itemize}

As discussed before, we use the title of articles as our context information. Thus it is only natural to find topical similarity between the article title and any passage from the article. So while estimating subtopic similarity between these passages, the deciding factors should be other topics than the title topic (context) which is represented by the difference vector between passage and context. We find evidence for this in our ablation study as it showed difference vectors to be more beneficial. 

\subsection{Qualitative study} We conduct a qualitative analysis on the clustering evaluation. The aim of this analysis is to explain why some of the methods we present here are better than the rest and to provide example cases from the dataset bolstering the explanation.

Consider the following pair of passages from Y1 test dataset:

\noindent\fbox{%
    \parbox{0.48\textwidth}{%
        \textbf{Passage 1: } Gardens in Renaissance were adorned with sculptures, topiary and fountains. In the 17th century, knot gardens became popular along with the hedge mazes. By this time, Europeans started planting new flowers such as tulips, marigolds and sunflowers. \\
        \textbf{Passage 2: } Islamic gardens were built after the model of Persian gardens and they were usually enclosed by walls and divided in 4 by watercourses. Commonly, the center of the garden would have a pool or pavilion. Specific to the Islamic gardens are the mosaics and glazed tiles used to decorate the rills and fountains that were built in these gardens.
    }%
}

From only the text of the passages, it is quite difficult to decide whether they are similar enough to share same subtopic cluster. In context of human culture they are topically distinct but in context of history of gardening, they are similar. Due to this ambiguity, $en-psg-cos$, which does not have access to the context information, assigned low similarity score to this passage pair. But the passage pair is taken from the Y1 test article titled \textit{"Gardening"} and share the \textit{"History (of gardening)"} subtopic. Access to this piece of information (article title) helps the CATS variant $en-psg-CATS$ to resolve the ambiguity and as a result it is able to correctly assign high similarity scores between them.

The previous example is a case of false negative which is rectified by the CATS approach. However, we find this to be rare instance across both the datasets and embedding methods. The dominant reason for CATS approach to achieve better results is its ability to identify false positive cases. Let us consider another pair of passages again from Y1 test dataset:

\noindent\fbox{%
    \parbox{0.48\textwidth}{%
        \textbf{Passage 1: } Big-game fishing started as a sport after the invention of the motorized boat.  In 1898, Dr. Charles Frederick Holder, a marine biologist and early conservationist, pioneered this sport and went on to publish many articles and books on the subject noted for their combination of accurate scientific detail with exciting narratives. \\
        \textbf{Passage 2: } In addition to capturing fish for food, recreational anglers might also keep a log of fish caught, either in a physical form or with technology such as the FISHBUOY or Fishbrain mobile logging application, and submit trophy-sized fish to independent record keeping bodies. In the Republic of Ireland, the Irish Specimen Fish Committee ...... It also uses a set of 'fair play' regulations to ensure fish are caught in accordance with accepted angling norms.
    }%
}

It is evident that these two passages share topics such as \textit{"fishing"}, \textit{"sport"}, and consequently influence non-CATS method such as $ewp-psg-cos$ to incorrectly assign high similarity score. However, when we are aware that both these passages are taken from an article titled \textit{"Recreational fishing"}, then other less overt topics become more important in deciding their subtopic similarity. As there are little topical overlap other than the central context, $ewp-psg-cos$ assigns low similarity score for the pair. Indeed these two passages are taken from different subtopics, \textit{"History"} and \textit{"Fish logs"} respectively. 

\subsection{Effect of Post-Processing and Dimensionality Reduction}
Mu et al. had shown that much of the variance of popular word embedding vectors are explained by only a small subset of vector dimensions. In their study, they develop a simple post-processing technique which made the embeddings more effective by eliminating those dominating embedding dimensions, as opposed to eliminating the weakest dimensions in case of typical dimensionality reduction techniques. According to their findings, their technique can be used to initialize embedding vectors for supervised downstream tasks. However, when used with our best performing CATS variant $q-ewp$, it does not yield any significant improvement.

All passage embeddings used for our evaluation are of length 768. Reducing the dimensionality of embeddings without significantly hurting the performance, benefit memory-constrained downstream applications. Raunak et al. propose a dimensionality reduction technique that combines PCA with Mu et al. post-processing algorithm, suitable for word embeddings. We explore the applicability of their technique to passage embeddings in comparison to simple PCA. Our findings, presented in Figure \ref{fig:dimred}, lead to interesting insights. As expected, for $ewp$ and $ews$, reducing dimensionality using normal PCA degrades performance in terms of AUC score for binary classification task. However, the performance degrades even further when we apply the technique by Raunak et al. On the other hand, for $en$ and $ens$, reduced embedding vectors produce improved results compared to the original vector. More interestingly, the improvement sustains even if the embedding size is reduced to only 100. Our conclusions from this observation are as follows:
\begin{itemize}
    \item The key difference between $en, ens$ and $ewp, ews$ is the incorporation of Wikipedia section information for the later two. The dimensionality reduction technique by Raunak et al. eliminates dominating dimensions similar to Mu et al. and eliminates weakest dimensions using PCA. As our evaluation results show, applying Raunak et al. method hurts the performance much more than a simple PCA dimensionality reduction. This suggests that important Wikipedia hierarchy information is concentrated in dominant dimensions of these passage embeddings and when those dimensions are eliminated, the performance degradation is much severe than simple PCA.
    \item For $en, ens$, dimensionality reduction results in improved performance and the improvement is slightly higher for Raunak et al. This finding aligns with the work of Mu et al. and Raunak et al. with word embeddings. Unlike $ewp, ews$, these embeddings do not have essential information concentrated in specific sets of dimensions and hence do not lose important insights upon removal of dominant or weak dimensions. In fact they benefit from denoising nature of these post-processing techniques.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{graphics/emb_vec_dim_red_exp.png}
  \caption{Effect of dimensionality reduction of passage embedding vectors compared to the original vector}
  \label{fig:dimred}
\end{figure}